{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d093fd-dee1-4a95-9b38-415ff67f693d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76075b64-bff3-466c-9d61-be04663ab69c",
   "metadata": {},
   "source": [
    "# 1. Install required packages (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09cd5c-cb74-43d7-8b14-3d6771c449ef",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers datasets peft accelerate\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4635d28b-38ae-4bfb-a70f-7acdfdc17f74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2. Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bac392-fbc5-4b33-bce7-9ccf1ea9b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4660c5-5752-41b7-9036-1cf7106cb23c",
   "metadata": {},
   "source": [
    "# 3. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fb8ab-070e-4960-8623-6950781089ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99041d5b-6301-4f92-a6fe-390173de41f1",
   "metadata": {},
   "source": [
    "# 4. Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99460d7-ca87-4d7d-9d5e-95acd21f319d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # for MCQ, format input and output text\n",
    "    input_text = example['question']\n",
    "    output_text = example['answer']\n",
    "    return {\"input\": input_text, \"output\": output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95b12a-ef23-4e8b-ac5e-d3cda63e9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71099980-4cd8-4d7c-a8ce-2442f604e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd9922-6654-4aa8-8f8e-ef4efdd3916d",
   "metadata": {},
   "source": [
    "# 5. Hugging face token for loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6c6ee-2556-4568-9d87-1acf2e8679c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "HF_TOEKN = os.getenv('HF_TOEKN')\n",
    "\n",
    "\n",
    "login(token=HF_TOEKN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e366f1-199c-4a80-8190-ca358dfc2f1c",
   "metadata": {},
   "source": [
    "# 6. Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7d747-e878-477f-897f-0576f57ad5c4",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\" # see if training becomes faster?\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# No need to save locally\n",
    "# model.save_pretrained(\"./llama-3b\")\n",
    "# tokenizer.save_pretrained(\"./llama-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8a8ff-9bbf-4f23-a630-b1273dda72c3",
   "metadata": {},
   "source": [
    "# 6. Apply LoRA PEFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb5b1e-8294-486d-a89c-ea6bb3a60545",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8131ea7-ce7b-4028-bc93-324680ecda8f",
   "metadata": {},
   "source": [
    "# 8. Tokenize inputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c05f3-a9ce-4202-b850-3f5b7b9d341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    # Prepare inputs (prompt + answer)\n",
    "    inputs = [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(examples[\"input\"], examples[\"output\"])]\n",
    "    tokenized = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # For causal LM, labels = input_ids (model shifts internally)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"Before\", train_dataset.column_names)\n",
    "train_dataset = train_dataset.map(tokenize_func, batched=True, remove_columns=train_dataset.column_names)\n",
    "print(\"After\", train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88b644-78fd-46aa-ac99-22887bc9d948",
   "metadata": {},
   "source": [
    "# 8. Setup training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f2a83-f554-4fff-b36a-41b094784654",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3b-lora-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    save_steps=30,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131db5a",
   "metadata": {},
   "source": [
    "# 9. Check if running on CPU OR GPU (For Apple silicon)\n",
    "If MPS available and MPS built are True and model_device shows mps, you are running on your Apple Silicon GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45555171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "print(\"Model device:\", model_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604150e-4a1e-4c3c-8a66-b7760a82b2c2",
   "metadata": {},
   "source": [
    "# 10. Create Trainer and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476049b-fd0b-4e96-b4cf-2e46bedeee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"fp16:\", training_args.fp16)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536bc98",
   "metadata": {},
   "source": [
    "# 11. Generate gguf file\n",
    "To create and use it with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf252929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"chat_nedicine\", tokenizer, quantization_method=\"f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38445d00",
   "metadata": {},
   "source": [
    "# 12. Download Model (for running on colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('/content/chat_doc_gpt_model/unsloth.F16.gguf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
